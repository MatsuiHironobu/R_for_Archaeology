@misc{Adolfo_2019,
  title = {Plumbers, Chains, and Famous Painters: {{The}} (Updated) History of the Pipe Operator in {{R}}},
  author = {{\'A}lvarez, Adolfo},
  year = {2021},
  month = sep,
  journal = {Adolfo {\'A}lvarez},
  urldate = {2024-05-27},
  abstract = {A modern, beautiful, and easily configurable blog theme for Hugo.}
}

@article{Anscombe_1973,
  title = {Graphs in {{Statistical Analysis}}},
  author = {Anscombe, Francis John},
  year = {1973},
  journal = {The American Statistician},
  volume = {27},
  number = {1},
  eprint = {2682899},
  eprinttype = {jstor},
  pages = {17--21},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2682899},
  urldate = {2024-05-06}
}

@article{Eerkens_2000,
  title = {Practice {{Makes Within}} 5\% of {{Perfect}}: {{Visual Perception}}, {{Motor Skills}}, and {{Memory}} in {{Artifact Variation}}},
  shorttitle = {Practice {{Makes Within}} 5\% of {{Perfect}}},
  author = {Eerkens, Jelmer},
  year = {2000},
  month = aug,
  journal = {Current Anthropology - CURR ANTHROPOL},
  volume = {41},
  pages = {663--668},
  doi = {10.1086/317394},
  file = {/Users/matsuihironobu/Zotero/storage/U8P5YQWR/Eerkens - 2000 - Practice Makes Within 5% of Perfect Visual Percep.pdf}
}

@article{Eerkens_Bettinger_2001,
  title = {Techniques for {{Assessing Standardization}} in {{Artifact Assemblages}}: {{Can We Scale Material Variability}}?},
  shorttitle = {Techniques for {{Assessing Standardization}} in {{Artifact Assemblages}}},
  author = {Eerkens, Jelmer W. and Bettinger, Robert L.},
  year = {2001},
  month = jul,
  journal = {American Antiquity},
  volume = {66},
  number = {3},
  pages = {493--504},
  publisher = {Cambridge University Press},
  issn = {0002-7316, 2325-5064},
  doi = {10.2307/2694247},
  urldate = {2021-11-21},
  abstract = {The study of artifact standardization is an important line of archaeological inquiry that continues to be plagued by the lack of an independent scale that would indicate what a highly variable or highly standardized assemblage should look like. Related to this problem is the absence of a robust statistical technique for comparing variation between different kinds of assemblages. This paper addresses these issues. The Weber fraction for line-length estimation describes the minimum difference that humans can perceive through unaided visual inspection. This value is used to derive a constant for the coefficient of variation (CV = 1.7 percent) that represents the highest degree of standardization attainable through manual human production of artifacts. Random data are used to define a second constant for the coefficient of variation that represents variation expected when production is random (CV = 57.7 percent). These two constants can be used to assess the degree of standardization in artifact assemblages regardless of kind. Our analysis further demonstrates that CV is an excellent measure of standardization and provides a robust statistical technique for comparing standardization in samples of artifacts.          ,              R{\'e}sum{\'e}             El estudio de estandarizaci{\'o}n y variaci{\'o}n ha sido una importante y valiosa linea de inter{\'e}s en los an{\'a}lisis arqueol{\'o}gicos. Sin embargo, a{\'u}n persisten dos problemas que son el enfoque de este estudio. En primer lugar, faltan medidas independientes para evaluar problemas de estandarizaci{\'o}n y variaci{\'o}n. En otros t{\'e}rmines, no hay nada que indica c{\'o}mo se debe hacer una muestra arqueol{\'o}gica bien estandardizada {\cyrchar\cyro} bien variable. En segundo lugar, no existe una t{\'e}cnica estadistica segura para hacer comparaciones cuantitativas. El 'Weberfraction,' utilizado para la estimaci{\'o}n de una l{\'i}nea amplia describe la diferencia m{\'i}nima que seres humanos pueden percibir con solo una inspecci{\'o}n ocular. Este valor es utilizado para derivar una constante (CV = 1.7percent) que repr{\'e}senta la variaci{\'o}n minima obtenida a trav{\'e}s de la producci{\'o}n manual de artefactos por seres humanos. Datas aleatorios son utilizados para determinar una segunda constante que repr{\'e}senta la variaci{\'o}n esperada bajo condiciones aleato-rias (CV = 57.7 percent). De este modo, estas dos constantes pueden estar utilizadas para determinar el grado de estandarizaci{\'o}n en las colecciones de artefactos. Tambi{\'e}n, este estudio proporciona una t{\'e}cnica estadistica segura para compararla estandarizaci{\'o}n en muestras de artefactos.}
}

@book{grolemund_2015,
  title = {{RStudioではじめるRプログラミング入門}},
  author = {Grolemund, Garrett},
  translator = {大橋, 真也 and 長尾, 高弘},
  year = {2015},
  month = mar,
  publisher = {オライリージャパン},
  address = {東京},
  abstract = {「統計を使わずに、Rを純粋にプログラミング言語として学ぼう」というコンセプトに基づいて書かれた本書は、統計学についての難しい知識がなくても、また、プログラミングの経験があまりなくても、プログラミングを学んでみようという意欲さえあれば読むことのできる、画期的な書籍です。サイコロを作る、トランプゲームを作る、スロットマシンを作る、という3つの簡単なプロジェクトに取り組む過程で、Rのプログラミング統合環境、RStudioを活用し、楽しみながら効率的にRのプログラミングスキルを身に付けていきます。統計だけに使うのはもったいない、Rのプログラミング言語としての隠れた魅力と可能性、さらにはデータサイエンスの基礎としてのデータ分析を紹介します。},
  isbn = {978-4-87311-715-7},
  langid = {japanese},
  yomi = {grolemund,garrett}
}

@article{ishii_2020,
  title = {{考古学のためのデータビジュアライゼーション}},
  author = {石井, 淳平},
  year = {2020},
  month = feb,
  journal = {奈良文化財研究所研究報告},
  number = {24},
  publisher = {独立行政法人国立文化財機構奈良文化財研究所},
  urldate = {2020-10-31},
  langid = {japanese},
  yomi = {ishii,junpei}
}

@misc{Marwick_2019,
  title = {Archaeological {{Science}} with {{R}}},
  author = {Marwick, Ben},
  year = {2019},
  month = may,
  urldate = {2024-05-20},
  abstract = {This is the book site for `Archaeological Science with R'.},
  howpublished = {https://benmarwick.github.io/aswr/}
}

@book{matsumura_yutani_2018,
  title = {{RユーザのためのRStudio[実践]入門-tidyverseによるモダンな分析フローの世界-}},
  author = {松村, 優哉 and 湯谷, 啓明 and 紀ノ定, 保礼 and 前田, 和寛},
  year = {2018},
  month = jun,
  publisher = {技術評論社},
  address = {東京},
  abstract = {RStudioを使ってデータ分析を効率化しよう!   RStudioはR言語のIDE(開発環境)です。 エディタ、コンソール、グラフなどを1つの画面内で確認できるほか、 データ分析プロジェクトをスムーズに進めるための機能が豊富に用意されているので、 RユーザにとってRStudioを利用したデータ分析はスタンダードになっています。  本書はRStudioの基本的な機能を解説したあとに、データ分析ワークフローを一通り解説していきます。 データの収集(2章)、データの整形(3章)、可視化(4章)、レポーティング(5章)など、 データ分析に欠かせないこれらの要素の基礎を押さえることができます。 また、本書はtidyverseパッケージを用いてこれらのデータ分析ワークフローを解説している側面を持ちます。 tidyverseの考えに触れ、モダンなデータ分析をはじめましょう。},
  isbn = {978-4-7741-9853-8},
  langid = {japanese},
  yomi = {matsumura,yuuya and yutani,hiroaki}
}

@article{muraki_2004a,
  title = {{石塔造立からみた惣墓の形成過程}},
  author = {村木, 二郎},
  year = {2004},
  month = feb,
  journal = {国立歴史民俗博物館研究報告 = Bulletin of the National Museum of Japanese History},
  volume = {111},
  pages = {147--159},
  publisher = {国立歴史民俗博物館},
  doi = {10.15024/00001201},
  urldate = {2024-05-26},
  abstract = {source:https://www.rekihaku.ac.jp/outline/publication/ronbun/ronbun5/index.html\#no111},
  langid = {japanese}
}

@article{muraki_2004b,
  title = {{石塔の多様化と消長 : 天理市中山念仏寺墓地の背光五輪塔から（第一部 地域社会におけるカミ祭祀と葬墓制）}},
  shorttitle = {{石塔の多様化と消長}},
  author = {村木, 二郎},
  year = {2004},
  month = feb,
  journal = {国立歴史民俗博物館研究報告 = Bulletin of the National Museum of Japanese History},
  volume = {112},
  pages = {25--62},
  publisher = {国立歴史民俗博物館},
  doi = {10.15024/00001216},
  urldate = {2024-05-26},
  abstract = {奈良県天理市中山念仏寺墓地には中世から現代に至る九千基を越える石塔が存在する。これらは、一六〜一七世紀は背光五輪塔、一八世紀は舟形、一九世紀は櫛形、二〇世紀は角柱形と、時代とともに主要形式が変化していく。なかでも二〇〇年にわたって立てられる背光五輪塔は、中世から近世への転換期に盛行し、惣墓（共同墓地）形成過程をたどる好資料である。そこで、本稿では背光五輪塔に着目し、まず三型式一八類に分類する。そしてそれを基礎に、他形式の舟形、櫛形との比較を通し、石塔の形式・型式が多様化する現象を捉える。次に、石塔の大きさ、刻まれた法名（戒名）の分析によって、石塔形式（型式）の違いは格差を表現しており、それは石塔の造立数が増加する一七世紀末から起こる現象であることを示す。すなわち、誰もが石塔を立てられなかった当初は、石塔を立てることによって格差を表現していた。しかし造立数が増えるにつれ、人とは違った石塔を立てることにより格差を表すようになるのである。庶民層の墓で捉えられたこの現象は、この時期に庶民層の階層分化が進んでいることを考古学的に示している。また、ひとつの石塔に書かれる法名の数（人数）を手がかりに、石塔が個人のものから複数人のものに変わっていくにつれ、背光五輪塔が消滅していくことを示す。石塔には宗教的側面と機能的側面がある。いずれも重要であるが、機能面により大きな要因があり、ある形式が消滅していく過程をたどる。 There are gravestones on more than 9,000 graves in the graveyard of the Nakayama Nembutsu temple in Tenri City, Nara Prefecture that date from the Medieval period through to the modern day. The main forms of the gravestones changed with the passing of time, as haiko-gorinto gravestones (gravestones depicting an image of five-tiered gravestones) were erected in the 16th and 17th centuries, boat-shaped gravestones in the 18th century, comb-shaped gravestones in the 19th century, and square pillar-shaped gravestones in the 20th century. Among these are haiko-gorinto gravestones that were built over a period of 200 years and were popular during the transitional period between the Medieval and Early Modern periods. These gravestones provide excellent source materials for tracing the process of the formation of communal graveyards. This paper focuses on the haiko-gorinto gravestones, firstly classifying them into three types and 18 different kinds. Then, on the basis of these classifications I discuss the phenomenon of the diversification in the form and types of gravestones by way of a comparison with the different boat-shaped and comb-shaped gravestones. Next, through an analysis of the size of the gravestones and the inscribed Buddhist names (posthumous Buddhist names) we learn that the differences in the shape (type) of gravestones demonstrate differences in social ranking, and that this phenomenon first occurred at the end of the 17th century when there was an increase in the number of gravestones erected. In other words, in the beginning when everyone was not able to erect a gravestone, the erection of a gravestone signified social ranking. However, as more and more gravestones were erected, people began to express their social ranking by erecting gravestones that were different from those of others. This phenomenon is seen in the graves of commoners, and shows in archeological terms the evolution of class differentiation that was occurring amongst commoners during this period. Further, the number of Buddhist names (people) inscribed on gravestones provides a useful clue as we learn that the more gravestones came to represent a number of people instead of an individual the more these haiko-gorinto gravestones began to disappear. Gravestones possess both a religious aspect and a functional aspect. While both are important, there are more contributing factors that affect the functional aspect, which helps trace the extinction of a particular form. source:https://www.rekihaku.ac.jp/outline/publication/ronbun/ronbun5/index.html\#no112},
  langid = {japanese}
}

@article{okumura_2013,
  title = {{「ネ申Excel」問題}},
  author = {奥村, 晴彦},
  year = {2013},
  month = aug,
  journal = {情報教育シンポジウム2013論文集},
  volume = {2013},
  number = {2},
  pages = {93--98},
  urldate = {2024-05-03},
  abstract = {情報学広場 情報処理学会電子図書館},
  langid = {japanese},
  yomi = {okumura,haruhiko},
  file = {/Users/matsuihironobu/Zotero/storage/PIN834KK/晴彦 - 2013 - 「ネ申Excel」問題.pdf}
}

@article{ozawa_1994,
  title = {{考古学における可視化}},
  author = {小澤, かおる},
  year = {1994},
  journal = {可視化情報学会誌},
  volume = {14},
  number = {53},
  pages = {71--73},
  doi = {10.3154/jvs.14.71},
  langid = {japanese},
  yomi = {ozawa,kaoru},
  file = {/Users/matsuihironobu/Zotero/storage/PN5EH8VV/小澤 - 1994 - 考古学における可視化.pdf}
}

@article{Rasch_et_al_2001,
  title = {The Two-Sample t Test: {{Pre-testing}} Its Assumptions Does Not Pay Off},
  shorttitle = {The Two-Sample t Test},
  author = {Rasch, Dieter and Kubinger, Klaus and Moder, Karl},
  year = {2011},
  month = feb,
  journal = {Stat. Pap.},
  volume = {52},
  pages = {219--231},
  doi = {10.1007/s00362-009-0224-x},
  abstract = {Traditionally, when applying the two-sample t test, some pre-testing occurs. That is, the theory-based assumptions of normal distributions as well as of homogeneity of the variances are often tested in applied sciences in advance of the tried-for t test. But this paper shows that such pre-testing leads to unknown final type-I- and type-II-risks if the respective statistical tests are performed using the same set of observations. In order to get an impression of the extension of the resulting misinterpreted risks, some theoretical deductions are given and, in particular, a systematic simulation study is done. As a result, we propose that it is preferable to apply no pre-tests for the t test and no t test at all, but instead to use the Welch-test as a standard test: its power comes close to that of the t test when the variances are homogeneous, and for unequal variances and skewness values {\textbar}{$\gamma$} 1{\textbar} {$<$} 3, it keeps the so called 20\% robustness whereas the t test as well as Wilcoxon's U test cannot be recommended for most cases. KeywordsPre-tests--Two-sample t test--Welch-test--Wilcoxon-U test}
}

@article{Riedel_et_al_2022,
  title = {Replacing Bar Graphs of Continuous Data with More Informative Graphics: Are We Making Progress?},
  shorttitle = {Replacing Bar Graphs of Continuous Data with More Informative Graphics},
  author = {Riedel, Nico and Schulz, Robert and Kazezian, Vartan and Weissgerber, Tracey},
  year = {2022},
  month = aug,
  journal = {Clinical Science},
  volume = {136},
  number = {15},
  pages = {1139--1156},
  issn = {0143-5221},
  doi = {10.1042/CS20220287},
  urldate = {2024-05-26},
  abstract = {Recent work has raised awareness about the need to replace bar graphs of continuous data with informative graphs showing the data distribution. The impact of these efforts is not known. The present observational meta-research study examined how often scientists in different fields use various graph types, and assessed whether visualization practices have changed between 2010 and 2020. We developed and validated an automated screening tool, designed to identify bar graphs of counts or proportions, bar graphs of continuous data, bar graphs with dot plots, dot plots, box plots, violin plots, histograms, pie charts, and flow charts. Papers from 23 fields (approximately 1000 papers/field per year) were randomly selected from PubMed Central and screened (n=227998). F1 scores for different graphs ranged between 0.83 and 0.95 in the internal validation set. While the tool also performed well in external validation sets, F1 scores were lower for uncommon graphs. Bar graphs are more often used incorrectly to display continuous data than they are used correctly to display counts or proportions. The proportion of papers that use bar graphs of continuous data varies markedly across fields (range in 2020: 4--58\%), with high rates in biochemistry and cell biology, complementary and alternative medicine, physiology, genetics, oncology and carcinogenesis, pharmacology, microbiology and immunology. Visualization practices have improved in some fields in recent years. Fewer than 25\% of papers use flow charts, which provide information about attrition and the risk of bias. The present study highlights the need for continued interventions to improve visualization and identifies fields that would benefit most.},
  file = {/Users/matsuihironobu/Zotero/storage/9CB95EN9/Riedel et al. - 2022 - Replacing bar graphs of continuous data with more .pdf}
}

@article{Sidiropoulos_et_al_2018,
  title = {{{SinaPlot}}: {{An Enhanced Chart}} for {{Simple}} and {{Truthful Representation}} of {{Single Observations Over Multiple Classes}}},
  shorttitle = {{{SinaPlot}}},
  author = {Sidiropoulos, Nikos and Sohi, Sina Hadi and Pedersen, Thomas Lin and Porse, Bo Torben and Winther, Ole and Rapin, Nicolas and Bagger, Frederik Otzen},
  year = {2018},
  month = jul,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {27},
  number = {3},
  pages = {673--676},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2017.1366914},
  urldate = {2024-05-26},
  abstract = {Recent developments in data-driven science have led researchers to integrate data from several sources, over diverse experimental procedures, or databases. This alone poses a major challenge in truthfully visualizing data, especially when the number of data points varies between classes. To aid the representation of datasets with differing sample size, we have developed a new type of plot overcoming limitations of current standard visualization charts. SinaPlot is inspired by the strip chart and the violin plot and operates by letting the normalized density of points restrict the jitter along the x-axis. The plot displays the same contour as a violin plot but resembles a simple strip chart for a small number of data points. By normalizing jitter over all classes, the plot provides a fair representation for comparison between classes with a varying number of samples. In this way, the plot conveys information of both the number of data points, the density distribution, outliers and data spread in a very simple, comprehensible, and condensed format. The package for producing the plots is available for R through the CRAN network using base graphics package and as geom for ggplot through ggforce. We also provide access to a web-server accepting excel sheets to produce the plots (http://servers.binf.ku.dk:8890/sinaplot/).},
  file = {/Users/matsuihironobu/Zotero/storage/A9PVQF8Q/Sidiropoulos et al. - 2018 - SinaPlot An Enhanced Chart for Simple and Truthfu.pdf}
}

@article{suzuki_suzumura_2015,
  title = {{データ可視化の必要性と意義: データビジュアライゼーションとは({$<$}特集{$>$}情報をわかりやすくするデザイン)}},
  shorttitle = {{データ可視化の必要性と意義}},
  author = {鈴木, 雅彦 and 鈴村, 嘉右},
  year = {2015},
  journal = {情報の科学と技術},
  volume = {65},
  number = {11},
  pages = {470--475},
  doi = {10.18919/jkg.65.11_470},
  abstract = {現代社会を生きる我々は,知らないうちに無数の情報に巡りあう。データの持っている「質」も複雑なものになり,それらのデータ群から「意味」を見出すのは困難を極める。近年,「ビッグデータ」という言葉がメディアに取り上げられて久しいが,データの集合自体はとても「無機質」なものである。そこから意味を見つけ,正しい解釈を行い,有効活用したいものの,それは専門的知識を有する一握りの特権となっている。万人が平等に,データから意味を見出すには,旧来のグラフでは,表現に限界があるだろう。見たい尺度で,別視点での描画を行う操作性・機能性を保持したグラフに出会うことは少ない。膨大なデータ群が今後も増えていく状況の中で,インタラクションを伴う「データビジュアライゼーション」と呼ばれる可視化技法について,その意義や手法について解説したい。},
  langid = {japanese},
  yomi = {suzuki,masahiko and suzumura,yoshisuke},
  file = {/Users/matsuihironobu/Zotero/storage/5N7JCVQK/鈴木 and 鈴村 - 2015 - データ可視化の必要性と意義 データビジュアライゼーションとは(情報をわかりやすくするデザイ.pdf}
}

@article{tukey_1962,
  title = {The {{Future}} of {{Data Analysis}}},
  author = {Tukey, John Wilder},
  year = {1962},
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {1},
  eprint = {2237638},
  eprinttype = {jstor},
  pages = {1--67},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2024-05-07}
}

@article{Wang_Marwick_2020,
  title = {Standardization of Ceramic Shape: {{A}} Case Study of {{Iron Age}} Pottery from Northeastern {{Taiwan}}},
  shorttitle = {Standardization of Ceramic Shape},
  author = {Wang, Li-Ying and Marwick, Ben},
  year = {2020},
  month = oct,
  journal = {Journal of Archaeological Science: Reports},
  volume = {33},
  pages = {102554},
  issn = {2352-409X},
  doi = {10.1016/j.jasrep.2020.102554},
  urldate = {2021-11-21},
  abstract = {The emergence of ceramic specialization in past societies is often linked to shifts in the complexity of social structures, because standardized ceramic production can reflect craft specialization and the presence of elite control. Previous work on identifying specialization relies on typological or linear metric analysis. Here we demonstrate how to investigate ceramic standardization by analyzing outlines of ceramic vessels. Outline analysis is useful because, unlike more commonly-used landmark analysis methods, it can effectively quantify shape differences for objects that lack distinctive measurement points needed for landmark analysis. We demonstrate this method using pottery from Kiwulan, a large multi-component Iron Age site (CE 1350--1850) in northeastern Taiwan. To measure ceramic specialization, we quantified pottery standardization by analyzing shape variables with reproducible geometric morphometric methods. We computed coefficients of variation (CVs) for shape coefficients obtained by elliptical Fourier analysis to test for shape standardization. We found significant differences in pottery shape and shape standardization that indicate changes in pottery production resulting from contact with mainland Han Chinese groups in northeastern Taiwan. Our case study, which includes an openly available research compendium of R code, represents an innovative application of outline-based methods in geometric morphometry to answer the anthropological questions of craft specialization.}
}

@misc{Wang_Marwick_2020_2,
  title = {Compendium of {{R}} Code and Data for {{Standardization}} of Ceramic Shape: {{A}} Case Study from the {{Iron Age}} Pottery from Northeastern {{Taiwan}}},
  author = {Wang, Li-Ying and Marwick, Ben},
  year = {2020},
  urldate = {2021-11-21},
  howpublished = {https://doi.org/10.17605/OSF.IO/ABVGF}
}

@misc{wickham_2014,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}} (3e)},
  author = {Wickham, Hadley and Navarro, Danielle and Pedersen, Thomas Lin},
  urldate = {2024-05-04},
  howpublished = {https://ggplot2-book.org}
}

@article{wickham_2014,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {59},
  pages = {1--23},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  urldate = {2024-05-04},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  copyright = {Copyright (c) 2013 Hadley  Wickham}
}

@book{wickham_2016,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley},
  year = {2016},
  series = {Use {{R}}!},
  edition = {2nd ed. 2016},
  publisher = {Springer International Publishing : Imprint: Springer},
  address = {Cham},
  doi = {10.1007/978-3-319-24277-4},
  abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to: - produce handsome, publication-quality plots with automatic legends created from the plot specification - superimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources with automatically adjusted common scales - add customizable smoothers that use powerful modeling capabilities of R, such as loess, linear models, generalized additive models, and robust regression - save any ggplot2 plot (or part thereof) for later modification or reuse - create custom themes that capture in-house or journal style requirements and that can easily be applied to multiple plots - approach a graph from a visual perspective, thinking about how each component of the data is represented on the final plot This book will be useful to everyone who has struggled with displaying data in an informative and attractive way. Some basic knowledge of R is necessary (e.g., importing data into R). ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page. New to this edition:{$<$} - Brings the book up-to-date with ggplot2 1.0, including major updates to the theme system - New scales, stats and geoms added throughout - Additional practice exercises - A revised introduction that focuses on ggplot() instead of qplot() - Updated chapters on data and modeling using tidyr, dplyr and broom},
  isbn = {978-3-319-24277-4},
  lccn = {519.5},
  file = {/Users/matsuihironobu/Zotero/storage/DAPWYLFQ/Wickham - 2016 - ggplot2 Elegant Graphics for Data Analysis.pdf}
}

@misc{wickham_2017,
  title = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2017},
  month = jan,
  urldate = {2024-05-04},
  howpublished = {https://r4ds.had.co.nz/}
}

@misc{wickham_2023,
  title = {R for {{Data Science}} (2e)},
  author = {Wickham, Hadley and {\c C}etinkaya-Rundel, Mine and Grolemund, Garrett},
  year = {2023},
  month = jul,
  urldate = {2024-05-04},
  howpublished = {https://r4ds.hadley.nz/}
}

@book{wickham_and_bryan_2023,
  title = {{R Packages: Organize, Test, Document, and Share Your Code}},
  shorttitle = {{R Packages}},
  author = {Wickham, Hadley and Bryan, Jenny},
  year = {2023},
  month = jul,
  edition = {第2版},
  publisher = {O'Reilly Media},
  abstract = {Turn your R code into packages that others can easily install and use. With this fully updated edition, developers and data scientists will learn how to bundle reusable R functions, sample data, and documentation together by applying the package development philosophy used by the team that maintains the "tidyverse" suite of packages. In the process, you'll learn how to automate common development tasks using a set of R packages, including devtools, usethis, testthat, and roxygen2.Authors Hadley Wickham and Jennifer Bryan from Posit (formerly known as RStudio) help you create packages quickly, then teach you how to get better over time. You'll be able to focus on what you want your package to do as you progressively develop greater mastery of the structure of a package.With this book, you will:Learn the key components of an R package, including code, documentation, and testsStreamline your development process with devtools and the RStudio IDEGet tips on effective habits such as organizing functions into filesGet caught up on important new features in the devtools ecosystemLearn about the art and science of unit testing, using features in the third edition of testthatTurn your existing documentation into a beautiful and user friendly website with pkgdownGain an appreciation of the benefits of modern code hosting platforms, such as GitHub},
  isbn = {978-1-09-813494-5},
  langid = {英語}
}

@article{wilkinson_1999,
  title = {Dot Plots},
  author = {Wilkinson, Leland},
  year = {1999},
  month = aug,
  journal = {American Statistician},
  volume = {53},
  number = {3},
  pages = {276--281},
  doi = {10.1080/00031305.1999.10474474},
  abstract = {Dot plots represent individual observations in a batch of data with symbols, usually circular dots. They have been used for more than 100 years to depict distributions in detail. Hand-drawn examples show their authors' efforts to arrange symbols so that they are as near as possible to their proper locations on a scale without overlapping enough to obscure each other. Recent computer programs that attempt to reproduce these historical plots have unfortunately resorted to simple histogram binning instead of using methods that follow the rules for the hand-drawn examples. This article introduces an algorithm that more accurately represents the dot plots cited in the literature.},
  file = {/Users/matsuihironobu/Zotero/storage/ME2A4AX9/Wilkinson - 1999 - Dot plots.pdf}
}
